# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #1 выполнил(а):
- Иванова Ивана Варкравтовна
- РИ000024
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Ознакомиться с основными операторами зыка Python на примере реализации линейной регрессии.

## Задание 1
### Найдите внутри C# скрипта “коэффициент корреляции ” и сделать выводы о том, как он влияет на обучение модели.
В данном случае коэффициент корреляции представляет из себя переменную distanceToTarget, а установленым у нее барьером - 1.42
```
  public override void OnActionReceived(ActionBuffers actionBuffers)
    {
        Vector3 controlSignal = Vector3.zero;
        controlSignal.x = actionBuffers.ContinuousActions[0];
        controlSignal.z = actionBuffers.ContinuousActions[1];
        rBody.AddForce(controlSignal * forceMultiplier);

        float distanceToTarget = Vector3.Distance(this.transform.localPosition, Target.localPosition);

        if(distanceToTarget < 1.42f)
        {
            SetReward(1.0f);
            EndEpisode();
        }
        else if (this.transform.localPosition.y < 0)
        {
            EndEpisode();
        }
    }
```
Чем меньше коофициент корреляции тем проше для МЛ агента становится задача, так как будет проще засчитать получение награды, однако модель отучится точнее. Обучение займет значительно дольше.


## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.
1.time_horizon - Количество шагов совершаемых агентом перед получением награды и добавлением в буфер опыта. При высоких значениях у агента выходит очень непостоянное вознаграждение, потому что он совершает очень много ненужных действий выполняя свою цель. При низких значениях в целом награда показывает себя ожидаемо стабильно. Но в случае если задача требовала от агента больше действий - времени могло не хватать и награда попросту оставалась бы на нуле.

2. extrinsic -> gamma - Определяет то насколько агент беспокоится о своих будующих наградах. К примеру, если параметр равен 1/2, то агенту важнее нынешняя награда в 2 раза больше чем следующая награда. По результатам тестирования, при большом значении этого параметра агент очень запоздало добирается до стабильного получения награды, однако при очень маленьком значении, агент хоть и быстрее в среднем получает награду, тратит большую часть своего времени на повторение старых ошибок и учится заторможено.
3. play_against_latest_model_ratio - Похожий на предыдущий параметр, у которого предпочтительнее ставить не границы(0;1) а находить оптимальное значение посередине. Отвечает за вероятность того, что нынешняя модель будет действовать в противовес предыдущей и пытаться кардинально менять свои действия. Если значение маленькое - модель замедляет свое обучение и редко отдаляется от пройденного пути, большое - модель наоборот ничему не учится и пытается очень хаотично добится желаемого

## Задание 3
###  Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения? 
Первый агент, который следует за обьектом, может быть полезен в создании npc которые автоматически атакуют передвигающегося противника, когда на того указал игрок или следуют за игроком в его "пати". Пример:(RDR2)

Второй агент может быть использован для создания жизненных циклов у NPС, чтобы мир вокруг игрока казался живой и не было такого, что все неигровые персонажи просто стоят на месте.

Лучше всего применять мл-агента там, где фиксированный скрипт не сработает - где мир окружающйи игрока постоянно меняется и создать мл-агента вместо прописывания мозга каждой букашки гораздо проще. Если мир фиксированный, более простым и не ресурсоемким решением будет не пользоваться ML-agent'ом

## Выводы
В ходе лабораторной работы было проведено ознакомелние с программным средством для создания системы машинного обучения ML-Agent и интегрирации его в Unity. Также была построена визуализация на основе логов в TebsorBoard

Цели лабораторной работы были достигнуты.

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
